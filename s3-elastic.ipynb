{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 Log File Processing and Elasticsearch Ingestion\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Read an incremental log file from S3\n",
    "2. Process and transform the data\n",
    "3. Push the processed data to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "from datetime import datetime\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "import urllib3\n",
    "\n",
    "# Suppress InsecureRequestWarning\n",
    "urllib3.disable_warnings(InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up your configuration parameters here. Make sure to set your environment variables for Elasticsearch credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Configuration\n",
    "bucket_name = 'your-bucket-name'\n",
    "file_key = 'path/to/your/logfile.log'\n",
    "\n",
    "# Elasticsearch Configuration\n",
    "es_host = os.environ.get('ES_HOST', 'https://localhost:9200')\n",
    "es_user = os.environ.get('ES_USER')\n",
    "es_password = os.environ.get('ES_PASSWORD')\n",
    "index_name = 'your-log-index'\n",
    "\n",
    "# File to store the last processed position\n",
    "state_file = 'last_processed_position.txt'\n",
    "\n",
    "# Verify Elasticsearch credentials\n",
    "if not all([es_host, es_user, es_password]):\n",
    "    raise ValueError(\"Elasticsearch credentials not properly set in environment variables.\")\n",
    "\n",
    "# Initialize S3 and Elasticsearch clients\n",
    "s3 = boto3.client('s3')\n",
    "es = Elasticsearch(\n",
    "    [es_host],\n",
    "    http_auth=(es_user, es_password),\n",
    "    scheme=\"https\",\n",
    "    port=9200,\n",
    "    verify_certs=False  # Set to True in production and provide proper certificates\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_position():\n",
    "    try:\n",
    "        with open(state_file, 'r') as f:\n",
    "            return int(f.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        return 0\n",
    "\n",
    "def save_last_position(position):\n",
    "    with open(state_file, 'w') as f:\n",
    "        f.write(str(position))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_logs():\n",
    "    last_position = get_last_position()\n",
    "    \n",
    "    # Get the file metadata\n",
    "    response = s3.head_object(Bucket=bucket_name, Key=file_key)\n",
    "    file_size = response['ContentLength']\n",
    "    \n",
    "    if last_position >= file_size:\n",
    "        print(\"No new data to process\")\n",
    "        return\n",
    "    \n",
    "    # Read only the new part of the file\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=file_key, Range=f'bytes={last_position}-{file_size}')\n",
    "    new_content = response['Body'].read().decode('utf-8')\n",
    "    \n",
    "    # Process new lines\n",
    "    for line in new_content.splitlines():\n",
    "        try:\n",
    "            log_entry = json.loads(line)\n",
    "            # Add a timestamp if not present\n",
    "            if 'timestamp' not in log_entry:\n",
    "                log_entry['timestamp'] = datetime.now().isoformat()\n",
    "            # Index the log entry\n",
    "            es.index(index=index_name, body=log_entry)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Skipping invalid JSON: {line}\")\n",
    "    \n",
    "    # Update the last processed position\n",
    "    save_last_position(file_size)\n",
    "    \n",
    "    print(f\"Processed {file_size - last_position} bytes of new log data\")\n",
    "\n",
    "# Run the log processing\n",
    "process_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Data in Elasticsearch\n",
    "\n",
    "You can use this cell to verify that data has been successfully pushed to Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for documents in the index\n",
    "search_result = es.search(index=index_name, body={\"query\": {\"match_all\": {}}})\n",
    "\n",
    "print(f\"Total documents in index: {search_result['hits']['total']['value']}\")\n",
    "print(\"Sample document:\")\n",
    "print(json.dumps(search_result['hits']['hits'][0]['_source'], indent=2) if search_result['hits']['hits'] else \"No documents found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
